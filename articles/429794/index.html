<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Google tells how the "exponential" growth of AI changes the very nature of computing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Google programmer Clif Young explains how the explosive development of depth learning algorithms coincides with the failure of Moore's law, the empiri...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">ğŸ”</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">ğŸ“œ</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">â¬†ï¸</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">â¬‡ï¸</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Google tells how the "exponential" growth of AI changes the very nature of computing</h1><div class="post__text post__text-html js-mediator-article"><h3>  Google programmer Clif Young explains how the explosive development of depth learning algorithms coincides with the failure of Moore's law, the empirical rule of computer chip progress for decades, and makes it develop fundamentally new computational schemes </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/f16/257/7ce/f162577cea1b28010ec340bfff65f307.jpg"><br><br>  The explosive development of AI and machine learning algorithms changes the very nature of computing - as they say in one of the largest AI practicing companies - on Google.  Google programmer Cliff Young spoke at the opening of the autumn microprocessor conference organized by the Linley Group, a popular symposium on computer chips held by a respected semiconductor analysis company. <br><br>  Yang said that the use of AI had moved into the â€œexponential phaseâ€ at the very moment when Mooreâ€™s law, the empirical rule of progress for computer chips, had been completely slowed down for decades. <br><a name="habracut"></a><br>  â€œThe times are pretty nervous,â€ he said thoughtfully.  â€œDigital CMOS is slowing down, we see problems with the 10nm process at Intel, we see them at the 7nm process from GlobalFoundries, and at the same time as the development of depth learning, an economic inquiry appears.  CMOS, a complementary metal-oxide-semiconductor structure, is the most common material used to make computer chips. <br><br>  While classic chips hardly increase efficiency and productivity, requests from AI researchers are growing, said Young.  He gave a bit of statistics: the number of machine learning papers stored on the arXiv preprint site maintained by Cornell University doubles every 18 months.  And the number of internal projects focusing on AI in Google, he said, also doubles every 18 months.  The need for the number of floating-point operations needed for processing neural networks used in machine learning is growing even faster - it doubles every three and a half months. <br><br>  All of this growth in computational queries is being combined into â€œMura's super law,â€ said Young, and he called this phenomenon â€œa bit intimidatingâ€ and â€œa little dangerousâ€, and â€œin order to worry about.â€ <br><br>  â€œWhere did all this exponential growth come from?â€ In the field of AI, he asked.  â€œIn particular, the thing is that in-depth training just works.  â€œIn my career, I have long ignored machine learning,â€ he said.  â€œIt was not obvious that these things would take off.â€ <br><br>  But then such breakthroughs quickly began to appear, such as pattern recognition, and it became clear that in-depth training was â€œincredibly effective,â€ he said.  â€œFor most of the last five years, we were a company that put AI in the first place, and we redid a large part of AI-based businesses,â€ from search to advertising and much more. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a72/3ff/1e9/a723ff1e9203c6ee395dc5cd8d8bd296.jpg"><br><br>  The Google Brain project team, the leading AI research project, needs â€œgiant machines,â€ said Young.  For example, neural networks are sometimes measured by the number of â€œweightsâ€ used in them, that is, the variables applied to a neural network and affect how it processes data. <br><br>  And if ordinary neural networks can contain hundreds of thousands or even millions of scales that need to be calculated, researchers from Google require themselves â€œtera-weight machinesâ€, that is, computers capable of counting trillions of scales.  Because "every time we double the size of a neural network, we improve its accuracy."  The rule of AI development is becoming more and more. <br><br>  In response to requests from Google, they are developing their own line of chips for MO, the Tensor Processing Unit.  TPU and its like are needed, since traditional CPU and GPU graphics chips do not cope with the loads. <br><br>  "We held ourselves together for a very long time and said that Intel and Nvidia are great at creating high-performance systems," said Young.  â€œBut we crossed this line five years ago.â€ <br><br>  TPU, after its first public appearance in 2017, caused hype by claims that it outperforms conventional chips in speed.  Google is already working on the third generation of TPU, using it in their projects and offering computer facilities on demand through the Google Cloud service. <br><br>  The company continues to manufacture TPU of all large and large sizes.  In its â€œstringedâ€ configuration, the 1024 TPUs are jointly connected to a new type of supercomputer, and Google plans to continue to expand this system, according to Young. <br><br>  â€œWe are creating giant multi-computers with tens of petabytes,â€ he said.  â€œWe are moving tirelessly on progress in several directions at once, and terabyte-scale operations continue to grow.â€  Such projects raise all the problems associated with the development of supercomputers. <br><br>  For example, Google engineers have adopted the tricks used in the legendary Cray supercomputer.  They combined a giant â€œmatrix multiplication moduleâ€, a part of the chip that carries the main computational burden for neural networks, with a â€œgeneral purpose vector moduleâ€ and a â€œgeneral purpose scalar moduleâ€, as was done in Cray.  â€œThe combination of scalar and vector modules allowed Cray to overtake all in performance,â€ he said. <br><br>  Google has developed its own innovative arithmetic constructions for programming chips.  A certain way of representing real numbers called bfloat16 improves the efficiency of processing numbers in neural networks.  In colloquial speech, it is called â€œbrain float numberâ€. <br><br>  TPU uses the fastest memory chips, high bandwidth memory, or HBM [high-bandwidth memory].  He said that the demand for large amounts of memory in the training of neural networks is growing rapidly. <br><br>  â€œMemory during training is used more intensively.  People talk about hundreds of millions of scales, but they have their own problems when processing the activation of "variables of the neural network. <br><br>  Google also tweaks a way of programming neural networks that helps squeeze the most out of hardware.  â€œWe are working on data and parallelism of the modelâ€ in such projects as â€œMesh TensorFlowâ€ - an adaptation of the TensorFlow software platform, â€œcombining data and parallelism on pod scalesâ€. <br><br>  Young did not disclose some technical details.  He noted that the company did not talk about internal connections, about how data travels around the chip - simply noted that "our connectors are gigantic."  He refused to cover this subject, which caused laughter in the audience. <br><br>  Young pointed to even more interesting areas of computation, which may soon be revealed to us.  For example, he suggested that calculations using analog chips, circuits that process input data in the form of continuous values â€‹â€‹instead of zeros and ones can play an important role.  "Perhaps we will turn to the analog field, in physics there is a lot of interesting things related to analog computers and NVM memory." <br><br>  He also expressed hope for the success of the startups associated with the chips presented at the conference: â€œThere are very cool startups here, and we need them to work, because the capabilities of digital CMOS are not limitless;  I want all these investments to shoot. â€ </div><p>Source: <a href="https://habr.com/ru/post/429794/">https://habr.com/ru/post/429794/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><p>Waiting for the list form <a href="../../index.html">here</a>...</p></nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>