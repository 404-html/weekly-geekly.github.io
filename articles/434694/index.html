<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>NeurIPS-2018 Review (ex. NIPS)</title>
  <meta name="description" content="In early December, the 32nd annual Neural Information Processing Systems conference was held in Montreal, dedicated to machine learning. According to ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="https://quick-geek.github.io/search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <section class="page js-page"><h1>NeurIPS-2018 Review (ex. NIPS)</h1><div class="post__text post__text-html js-mediator-article">  In early December, the 32nd annual <a href="https://nips.cc/">Neural Information Processing Systems</a> conference was held in Montreal, dedicated to machine learning.  According to the unofficial table of ranks, this conference is the top-1 event of a similar format in the world.  All conference tickets this year were sold out in a record 13 minutes.  We have a large data scientist team of MTS, but only one of them - Marina Yaroslavtseva ( <a href="https://habr.com/ru/users/magoli/" class="user_link">magoli</a> ) - was lucky enough to get to Montreal.  Together with Danila Savenkov ( <a href="https://habr.com/ru/users/danila_savenkov/" class="user_link">danila_savenkov</a> ), who was left without a visa and followed the conference from Moscow, we will tell you about the works that we found most interesting.  This sample is very subjective, but we hope it will interest you. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c10/868/6af/c108686af497e18c338a44c475d6d64e.png" alt="image"><br><a name="habracut"></a><br>  <b>Relational recurrent neural networks</b> <br><br>  <a href="https://arxiv.org/abs/1806.01822">Abstract</a> <br>  <a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/%2520relational_memory.py">Code</a> <br><br>  When working with sequences it is often very important how the elements of a sequence are related to each other.  Standard recurrent network architectures (GRU, LSTM) can hardly model the relationship between two elements that are sufficiently distant from each other.  To some extent, attention ( <a href="https://youtu.be/SysgYptB198">https://youtu.be/SysgYptB198</a> , <a href="https://youtu.be/quoGRI-1l0A">https://youtu.be/quoGRI-1l0A</a> ) helps to cope with this, but still it is not quite that.  Attention allows you to determine the weight with which the hidden state from each of the steps in the sequence will affect the final hidden state and, accordingly, the prediction.  We are interested in the interrelation of the elements of the sequence. <br><br>  Last year, again on the NIPS, google suggested to completely abandon recurrence and use <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need">self-attention</a> .  The approach proved to be very good, though mostly on seq2seq tasks (the article presents the results of machine translation). <br><br>  The authors of this year‚Äôs article use the idea of ‚Äã‚Äãself-attention as part of LSTM.  There are not many changes: <br><br><ol><li>  Change the cell state vector to the ‚Äúmemory‚Äù matrix M. To some extent, the memory matrix is ‚Äã‚Äãa lot of cell state vectors (many memory cells).  Obtaining a new element of the sequence, we determine how this element should update each of the memory cells. </li><li>  For each element of the sequence, we will update this matrix using multi-head dot product attention (MHDPA, you can read about this method in the above mentioned article from google).  The MHPDA result for the current element of the sequence and the matrix M is run through a fully meshed mesh, a sigmoid, and then the matrix M is updated in the same way as the cell state in LSTM </li></ol><br>  It is argued that it is through MHDPA that the grid can take into account the interrelation of the elements of the sequence even when they are separated from each other. <br><br>  As a toy problem, the model is asked to find the Nth vector in the sequence of vectors by the distance from the Mth in terms of the Euclidean distance.  For example, there is a sequence of 10 vectors and we ask to find one that is in third place in proximity to the fifth.  It is clear that to answer this question, the model needs to somehow estimate the distances from all the vectors to the fifth and sort them.  Here, the model proposed by the authors confidently wins LSTM and <a href="https://en.wikipedia.org/wiki/Differentiable_neural_computer">DNC</a> .  In addition, the authors compare their model with other architectures at the Learning to Execute (we receive several lines of code as input, issue the result), Mini-Pacman, Language Modeling, and everywhere report on the best results. <br><br>  <b>Multivariate Time Series Imputation with Generative Adversarial Networks</b> <br><br>  <a href="https://papers.nips.cc/paper/7432-multivariate-time-series-imputation-with-generative-adversarial-networks">Abstract</a> <br>  <a href="https://github.com/Luoyonghong/Multivariate-Time-Series-Imputation-with-Generative-Adversarial-Networks">Code</a> (although the article does not link here) <br><br>  In multidimensional time series, as a rule, there are a large number of gaps, which hinders the use of advanced statistical methods.  Standard solutions - filling with the middle / zero, removing incomplete cases, recovering data based on matrix expansions in this situation often do not work, because they cannot reproduce the time dependencies and the complex distribution of multidimensional time series. <br><br>  The ability of generative-adversary networks (GANs) to imitate any distribution of data, in particular, in the tasks of ‚Äúdrawing‚Äù persons and generating sentences, is widely known.  But, as a rule, such models either require initial training in full dataset without gaps, or do not take into account the consistent nature of the data. <br><br>  The authors propose to supplement GAN with a new element - the Gated Recurrent Unit for Imputation (GRUI).  The main difference from the usual GRU is that GRUI can be trained on data with intervals of different lengths between observations and correct the influence of observations depending on their distance in time from the current point.  A special attenuation parameter Œ≤ is calculated, the value of which varies from 0 to 1 and the smaller, the longer the time lag between the current observation and the previous non-empty one. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ab/c71/63b/6abc7163bf3ff1b16310104100b53236.png" alt="image"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/203/a64/599/203a6459932c852ad827db0d9574ef2c.png" alt="image"><br><br>  Both the discriminator and the GAN generator consist of a GRUI layer and a fully connected layer.  As usual in the GANs, the generator learns to imitate the original data (in this case, just fill in the gaps in the rows), and the discriminator learns to distinguish the rows filled with the generator from the real ones. <br><br>  As it turned out, such an approach very adequately recovers data even in time series with a very large proportion of gaps (in the table below - MSE data recovery in the KDD data, depending on the fraction of gaps and recovery method. In most cases, the GAN-based method gives the greatest accuracy recovery). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/188/737/e57/188737e575173058d1c0a79482b8679d.png" alt="image"><br><br>  <b>On the Dimensionality of Word Embeddings</b> <br><br>  <a href="https://arxiv.org/abs/1812.04224">Abstract</a> <br>  <a href="https://github.com/ziyin-dl/word-embedding-dimensionality-selection">Code</a> <br><br>  Word embedding / vector word representation is an approach widely used for various NLP applications: from recommender systems to analyzing the emotional coloring of texts and machine translation. <br><br>  At the same time, the question of how to optimally set such an important hyperparameter as the dimension of vectors remains open.  In practice, it is most often selected by empirical brute force or set by default, for example, at the level of 300. At the same time, too small dimension does not reflect all significant interrelationships between words, and too large can lead to retraining. <br><br>  The authors of the study propose their solution to this problem by minimizing the PIP loss parameter, a new measure of the difference between the two embedding options. <br>  The calculation is based on PIP-matrices, which contain the scalar products of all pairs of vector representations of words in the body.  PIP loss is calculated as the Frobenius norm between the PIP-matrices of two embeddings: trained on data (trained embedding E_hat) and perfect, trained on non-noisy data (oracle embedding E). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/135/a0a/1f2/135a0a1f216137c64ab73f9d2b86f0dc.png" alt="image" width="300" height="200"></div><br><br>  It would seem that everything is simple: you need to choose a dimension that minimizes PIP loss, the only incomprehensible point is where to get the oracle embedding.  In 2015-2017, a number of papers were published in which it was shown that various methods for constructing embeddings (word2vec, GloVe, LSA) implicitly factorize (lower the dimension) the signal matrix of the case.  In the case of word2vec (skip-gram), the signal matrix is <a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information">PMI</a> , in the case of GloVe, the log-counts matrix.  It is proposed to take a dictionary of very large size, build a signal matrix and use SVD to obtain oracle embedding.  Thus, the dimension of oracle embedding is obtained equal to the rank of the signal matrix (in practice, for a dictionary of 10k words, the dimension will be about 2k).  However, our empirical signal matrix is ‚Äã‚Äãalways noisy and we have to resort to clever schemes to get oracle embedding and evaluate the PIP loss using a noisy matrix. <br><br>  The authors argue that to select the optimal embedding dimension it is enough to use a dictionary of 10k words, which is not very much and allows you to get rid of this procedure in a reasonable time. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/454/806/a20/454806a208dd7c10df8b9cbd365c440a.png" alt="image"><br><br>  As it turned out, the embedding dimension thus calculated in most cases with an error of up to 5% coincides with the optimal dimension, determined on the basis of expert estimates.  It turned out (expectedly) that Word2Vec and GloVe practically do not retrain (PIP loss on very large dimensions does not fall), but LSA retrains quite strongly. <br><br>  With the help of the code laid out by the authors, you can search for the optimal dimension of Word2Vec (skip-gram), GloVe, LSA. <br><br>  <b>FRAGE: Frequency-Agnostic Word Representation</b> <br><br>  <a href="https://papers.nips.cc/paper/7408-frage-frequency-agnostic-word-representation">Abstract</a> <br>  <a href="https://github.com/ChengyueGongR/Frequency-Agnostic">Code</a> <br><br>  The authors talk about how embeddingings work differently for rare and for popular words.  By popular, I mean not stop words (we don‚Äôt consider them at all), but meaningful words that are not very rare. <br><br>  The observations are as follows: <br><br>  If we talk about popular words, then their proximity is cosine very well reflected. <br><br><ol><li>  their semantic intimacy.  For rare words, this is not the case (which is expected), and (which is less expected), the top-n of the closest in cosine measure of words to a rare word is also rare and semantically unrelated.  That is, rare and frequent words in the space of embeddingings live in different places (in different cones, if we are talking about cosine) </li><li>  During training, vectors of popular words are updated much more often and on average are twice as far from initialization than vectors for rare words.  This leads to the fact that the embeddingings of rare words are on average closer to the origin.  I, frankly, always thought that, on the contrary, the embeddingings of rare words are, on average, <a href="https://stackoverflow.com/a/50550236/6066878">very long</a> , and I don‚Äôt know how to treat the authors ‚Äôstatement =) </li></ol><br>  Whatever the case with the ratio of embeddingd L2-norms, the separability of popular and rare words is not a very good phenomenon.  We want the embeddings to reflect the semantics of the word, not its frequency. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c7e/f4d/e70/c7ef4de7034a8347269fa40d6bdc7005.png" alt="image"><br><br>  The picture shows popular Word2Vec (red) and rare (blue) words after SVD.  Under popular here refers to the top 20% of the words in frequency. <br><br>  If the problem was only in the L2-norms of embeddings, we could normalize them and live happily, but, as I said in the first paragraph, in cosine proximity (in polar coordinates) rare words are also separated from popular ones. <br><br>  The authors propose, of course, the GAN.  Let's do everything the same as before, but add a discriminator that will try to distinguish popular words from rare ones (again, we consider the top n% words in frequency to be popular). <br><br>  It looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f0e/0a3/0b7/f0e0a30b77a9071ff63fa208401fad56.png" alt="image"><br><br>  The authors test the approach on the tasks of word similarity, machine translation, text classification and language modeling and everywhere perform better baseline.  In word similarity it is argued that quality grows especially noticeably in rare words. <br><br>  One example: citizenship.  Skip-gram issues: bliss, pakistans, dismiss, reinforces.  FRAGE issues: population, st√§dtischen, dignity, b√ºrger.  The words citizen and citizens at FRAGE at 79 and 7 place, respectively (in proximity to the citizenship), at skip-gram - are not in the top 10000. <br><br>  For some reason, the authors posted the code only for the tasks of machine translation and language modeling, word similarity and text classification in the repository, unfortunately, are not represented. <br><br>  <b>Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces</b> <br><br>  <a href="http://papers.nips.cc/paper/7965-unsupervised-cross-modal-alignment-of-speech-and-text-embedding-spaces">Abstract</a> <br>  Code: no code, but I would like to <br><br>  Recent studies have shown that two vector spaces, trained using embedding algorithms (for example, word2vec) on text boxes in two different languages, can be matched to each other without marking and matching the contents of the two bodies to each other.  In particular, this approach is used for machine translation in the company Facebook.  One of the key properties of embedding spaces is used: inside them, similar words must be geometrically close, and unlike - on the contrary, be far from each other.  It is assumed that, in general, the structure of the vector space is preserved regardless of the language in which the corpus was used for learning. <br><br>  The authors of the article went further and applied a similar approach to the field of automatic speech recognition and translation.  It is proposed to teach the vector space separately for the text corpus in the language of interest (for example, Wikipedia), separately for the corpus of recorded speech (in audio format), perhaps in another language, previously broken into words, and then to compare these two spaces similarly to the two text enclosures. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d07/950/b6f/d07950b6f9bac05e9ce48dd80bb24838.png" alt="image"><br><br>  For text body, word2vec is used, and for speech, a similar approach, called Speech2vec, based on LSTM and methodologies used for word2vec (CBOW / skip-gram), so it is assumed that it combines words according to contextual and semantic characteristics, and not sounding. <br><br>  After both vector spaces are trained and there are two sets of embeddings - S (on the body of speech), consisting of n embeddings of dimension d1 and T (on the body of text), consisting of m embeddings of dimension d2, you need to match them.  Ideally, we have a dictionary that determines which vector from S corresponds to which vector from T. Then two matrices are formed for comparison: from S we choose k embeddings that form a matrix X of size d1 xk;  from T, too, k embeds are chosen that correspond (in the dictionary) to those previously selected from S, and a matrix Y of size d2 x k is obtained.  Next, you need to find a linear mapping W such that: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/db3/59e/e40/db359ee40ad9f907c6461d378db3d7aa.png" alt="image" width="300" height="200"></div><br><br>  But since the article discusses the unsupervised approach, there is initially no dictionary, so the procedure for generating a synthetic dictionary, consisting of two parts, is proposed.  First, we get the first approximation of W using the domain-adversarial training (competitive model like GAN, only instead of the generator - linear display W, with which we try to make S and T indistinguishable from each other, and the discriminator tries to determine the real origin of embedding).  Then, based on the words, the embeddingings of which showed the best fit to each other and are most often found in both cases, a dictionary is formed.  After this, W is refined according to the formula above. <br><br>  This approach gives results comparable to learning on labeled data, which can be very useful in the problem of speech recognition and translation from rare languages ‚Äã‚Äãfor which there are too few parallel speech-text shells, or they are missing. <br><br>  <b>Deep Anomaly Detection Using Geometric Transformations</b> <br><br>  <a href="https://arxiv.org/abs/1805.10917">Abstract</a> <br>  <a href="https://github.com/izikgo/AnomalyDetectionTransformations">Code</a> <br><br>  An unusual approach to anomaly detection, which, according to the authors, greatly overcomes other approaches. <br><br>  The idea is this: let's invent K different geometric transformations (combinations of shifts, rotate 90 degrees and reflections) and apply them to each picture of the original dataset.  The picture resulting from the i-th transformation will now belong to class i, that is, there will be K classes in total, each of them will be represented by such a number of pictures that was originally in dataset.  Now we will teach a multi-class classification on such markup (the authors chose wide resnet). <br><br>  Now we can get K vectors y (Ti (x)) of dimension K, where Ti is the i-th transformation, x is the picture, y is the output of the model.  The basic definition of ‚Äúnormality‚Äù is as follows: <br><br>  Here, for the image x, we add the predicted probabilities of the correct classes for all transformations.  The greater the ‚Äúnormality‚Äù, the more likely it is that the image is taken from the same distribution as the training sample.  The authors claim that it already works very well, but nevertheless they offer a more complex way that works even better.  We will assume that the vector y (Ti (x)) for each transformation Ti is distributed over <a href="https://youtu.be/nfBNOWv1pgE">Dirichlet</a> and as a measure of the ‚Äúnormality‚Äù of the image we will consider the log likelihood.  The Dirichlet distribution parameters are estimated on the training set. <br><br>  The authors report an incredible boost of performance compared to other approaches. <br><br>  <b>A Simple Unified Framework for Detecting Out of Distribution and Adversarial Attacks</b> <br><br>  <a href="http://papers.nips.cc/paper/by-source-2018-3551">Abstract</a> <br>  <a href="https://github.com/pokaxpoka/deep_Mahalanobis_detector">Code</a> <br><br>  Identifying cases in a sample for applying a model that is significantly different from the distribution of a training sample is one of the basic requirements for obtaining reliable classification results.  At the same time, neural networks are known for their feature with a high degree of certainty (and incorrectly) to classify objects that have not been encountered in training, or that are intentionally destructive (adversarial examples). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1e0/780/f35/1e0780f3540fcb4ba894f38a40c31cbb.png" alt="image"><br><br>  The authors of the article propose a new method of identifying both those and other "bad" cases.  The approach is implemented as follows: first, the neural network is trained with the usual softmax output, then the output of its penultimate layer is taken, and the generative classifier is trained on it.  Let there be x - what is fed to the input of the model for a specific classification object, y is the corresponding class label, then suppose that we have a pre-trained softmax classifier of the form: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/223/898/826/223898826445e2677c03a2078b182208.png" alt="image" width="300" height="200"></div><br><br>  Where wc and bc are weights and softmax layer constant for class c, and f (.) Is the output of the last but one soybean DNN. <br><br>  Further, without any changes to the pre-trained classifier, a transition is made to the generative classifier, namely discriminant analysis.  It is assumed that the signs taken from the penultimate layer of the softmax classifier have a multidimensional normal distribution, each component of which corresponds to one class.  Then the conditional distribution can be defined through the vector of averages of the multidimensional distribution and its covariance matrix: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ada/0d8/e70/ada0d8e70536c7993a6061b154f3c0eb.png" alt="image" width="300" height="200"></div><br><br>  To estimate the parameters of the generative classifier, empirical averages are calculated for each class, as well as the covariance for cases from the training set {(x1, y1), ..., (xN, yN)}: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ada/0d8/e70/ada0d8e70536c7993a6061b154f3c0eb.png" alt="image" width="300" height="200"></div><br><br>  where N is the number of cases of the corresponding class in the training sample.  Then, on the test sample, a measure of reliability is calculated - the Mahalanobis distance between the test case and the normal distribution of the class closest to this case. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/095/928/03b/09592803b18f664fad5940fc81d47f77.png" alt="image" width="400" height="300"></div><br><br>  As it turned out, such a metric works much more reliably on atypical or spoiled objects, without giving overestimated estimates, as in the softmax layer.  In most comparisons on different data, the proposed method showed results that exceeded the current state-of-the-art in finding both cases that were not in training, and intentionally spoiled. <br><br>  Next, the authors consider another interesting application of their methodology: use the generative classifier to highlight new classes on the test, which were not in training, and then update the parameters of the classifier itself so that it can later determine this new class. <br><br>  <b>Adversarial Examples that Fool both Computer Vision and Time-Limited Humans</b> <br>  Abstract: <a href="https://arxiv.org/abs/1802.08195">https://arxiv.org/abs/1802.08195</a> <br><br>  The authors investigate what adversarial examples are from the point of view of human perception.  Today, no one is surprised by the fact that it is possible almost without changing the image to make the network make mistakes on it.  However, it is not very clear how the original picture differs from the adversarial example for a person and if it differs at all.  It is clear that none of the people will call the picture on the right an ostrich, but still, perhaps the picture on the right for a person is not exactly identical to the picture on the left, and, if so, a person can also be exposed to adversarial attacks. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/859/b13/cf0/859b13cf0bb3c14ac58eeab39b5a0945.png" alt="image"><br><br>  The authors try to assess how well a person can classify adversarial examples.  To obtain adversarial examples, a technique is used which does not have access to the architecture of the original network (the authors' logic is that they still will not be given access to the architecture of the human brain). <br><br>  So, a person is shown adversarial example, as in the picture above and asked to classify it.  It is clear that in normal conditions the result would be predictable, but here one image is shown to a person for 63 milliseconds after which he must choose one of two classes.  In such conditions, the accuracy of the original images was 10% higher than the adversarial.  In principle, this could be explained by the fact that the adversarial image is simply noisy and therefore, in the conditions of a shortage of time, people incorrectly classify it, but this refutes the following experiment.  If, before adding perturbation to the image, we reflect this perturbation vertically, then accuracy will hardly change compared to the original image. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/966/f4d/e03/966f4de0389283dd83f73a1be2cf36cb.png" alt="image"><br><br>  On the histogram adv - adversarial example, image - the original image, flip - the original image + adversarial perturbation, reflected vertically. <br><br>  <b>Sanity Checks for Saliency Maps</b> <br><br>  <a href="https://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps">Abstract</a> <br><br>  Interpretation of models is one of the most discussed topics today.  If we are talking about deep learning, we usually talk about saliency maps.  Saliency maps try to answer the question of how the value changes at one of the grid outputs when the input values ‚Äã‚Äãchange.  This may look like a saliency map, which shows which pixels affected the image being classified as a ‚Äúdog‚Äù. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aa5/2c5/659/aa52c5659d4c00d7666661797671c4b9.png" alt="image"><br><br>  The authors ask a very reasonable question: ‚ÄúHow would we validate the methods of constructing saliency maps?‚Äù Two obvious theses are put forward that are proposed to be tested: <br><br><ol><li>  Saliency map must depend on grid weights. </li><li>  Saliency map should depend on the laws that are in the data </li></ol><br>  We will check the first thesis, replacing the weights in the trained grid with randomization: cascading randomization (random layers, starting with the last one and see how the saliency map changes) and independent randomization (random random layers).  The second thesis will be checked like this: randomly we mix all the labels on the train, overfit the train and look at the saliency maps. <br>  If the method of constructing a saliency map is really good and allows you to understand how the model works, such randomization should greatly change the saliency maps.  However: ‚ÄúTo our surprise, some widely deployed,‚Äù the authors state.  Here, for example, how saliency maps, obtained using various algorithms, after cascading randomization look like: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/965/eaa/ca7/965eaaca7902d24ef5369a63d493b4ff.png" alt="image"><br><br>  Note the funny fact that the last column corresponds to a grid with random weights in all layers.  That is, the grid predicts randomly, but some saliency maps still draw a bird. <br><br>  The authors rightly say that - assessment of saliency maps by their comprehensibility and consistency and insufficient attention to the extent to which the result is related to how the model works leads to a confirmation bias.  Apparently, including for this reason, it turns out that common approaches to the interpretation of models do not interpret them at all. <br><br>  <b>An intriguing failing of convolutional neural networks and the CoordConv solution</b> <br>  Abstract: <a href="https://arxiv.org/abs/1807.03247">https://arxiv.org/abs/1807.03247</a> <br>  Code: there are already many implementations, and in general the idea is so beautiful and simple that it is written literally in 10 lines. <br><br>  A simple and promising idea from Uber.  Convolutional networks are initially sharpened for shear invariance, so the tasks associated with determining the coordinates of an object for such networks are very difficult.  Ordinary convolutional networks are not able to solve even toy problems such as determining the coordinates of a point in a picture or drawing a point along coordinates: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ed/408/c14/6ed408c1456a509c86a6260df5c5a23b.png" alt="image"><br><br>  A very elegant hack is proposed: add to the picture (in general, to the CoodrConv layer input) two matrices i and j, which will contain the vertical and horizontal coordinates of the corresponding pixels: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2a/87e/8cb/a2a87e8cb6b7d283396a0fb3ef11fe30.png" alt="image"><br><br>  It is alleged that: <br><br><ol><li>  Such convolutions do not spoil the performance on ImageNet.  The task of classifying images just requires shift invariance, and, judging by the results, the grid easily learns that you don‚Äôt need to use coordinates in the classification task </li><li>  CoordConv greatly improves object detection.  According to the results of experiments on the detection of numbers from MNIST, scattered throughout the picture with the help of Faster R-CNN, declare an increase in IoU by 21% </li><li>  When using CoordConv in GAN, the variety of generated images increases. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fee/a18/425/feea18425335abbb7d784669a1b82bcd.png" alt="image"><br><br>  We trained GANs in two datasets: a toy dataset with red and blue figures in random places and a dataset with LSUN bedroom interiors.  In the first dataset, the centers of the figures are evenly distributed, the distribution of the difference between the centers is the upper graph in column c.  Given that the samples generated by GAN'om are similar to the original ones, it can be seen that the distribution of centers and distances between the centers is very different.  When using CoordConv, the distribution of distances between centers is much more similar to the original, although problems with distance differences remain.  In the case of LSUN on column d, it can be seen that, in general, the distribution of samples generated by the CoordConv GAN is closer to the original <br></li><li>  4. Using CoordConv in <a href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752">A2C</a> gives a boost in some (not all) games. </li></ol><br>  Personally, the second point is more interesting to me from this, I would like to see the results on real datasets, but at the same time nothing is googled.  Meanwhile, CoordConv is already actively building in <a href="https://youtu.be/81AvQQnpG4Q">U-net</a> : <a href="https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/69274">https://arxiv.org/abs/1812.01429, https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/69274</a> , <a href="https://github.com/mjDelta/Kaggle-RSNA-Pneumonia-Detection-Challenge">https: //github.com/mjDelta/Kaggle-RSNA-Pneumonia-Detection-Challenge</a> . <br><br>  There is a good and more detailed <a href="https://youtu.be/gMGL-shl3P8">video from the author</a> . <br><br>  <b>Regularizing by the Variance of the Activations' Sample-Variances</b> <br><br>  <a href="https://arxiv.org/abs/1811.08764">Abstract</a> <br>  <a href="https://github.com/avirambh/vcl_pytorch">Code</a> <br><br>  The authors offer a fun alternative to batch normalization.  We will fine the grid for the variability of the dispersion of activations on some layer. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –æ–Ω–∏ —Ä–µ–∞–ª–∏–∑—É—é—Ç —ç—Ç–æ —Ç–∞–∫: –≤–æ–∑—å–º–µ–º –∏–∑ –±–∞—Ç—á–∞ –¥–≤–∞ –Ω–µ–ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ S1 –∏ S2 –∏ –ø–æ—Å—á–∏—Ç–∞–µ–º —Ç–∞–∫—É—é —à—Ç—É–∫—É: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e5f/2ea/97a/e5f2ea97aaebf6af472880bc2190b2cc.png" alt="image" width="300" height="200"></div><br><br> –≥–¥–µ œÉ2 ‚Äî —ç—Ç–æ –≤—ã–±–æ—Ä–æ—á–Ω—ã–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –≤ S1 –∏ S2 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, Œ≤ ‚Äî –æ–±—É—á–∞–µ–º—ã–π –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç. –≠—Ç–∞ —à—Ç—É–∫—É –∞–≤—Ç–æ—Ä—ã –Ω–∞–∑—ã–≤–∞—é—Ç variance constancy loss (VCL) –∏ –¥–æ–±–∞–≤–ª—è—é—Ç –∫ –æ–±—â–µ–º—É –ª–æ—Å—Å—É. <br><br> –í —Å–µ–∫—Ü–∏–∏ –ø—Ä–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∞–≤—Ç–æ—Ä—ã –∂–∞–ª—É—é—Ç—Å—è –Ω–∞ —Ç–æ, –∫–∞–∫ –Ω–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥—è—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á—É–∂–∏—Ö —Å—Ç–∞—Ç–µ–π –∏ –∫–æ–º–º–∏—Ç—è—Ç—Å—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–ª–æ–∂–∏—Ç—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–π –∫–æ–¥ (–≤—ã–ª–æ–∂–∏–ª–∏). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª–∏ —Å–Ω–∞—á–∞–ª–∞ —Å –º–∞–ª–µ–Ω—å–∫–æ–π 11-—Å–ª–æ–π–Ω–æ–π —Å–µ—Ç–æ—á–∫–æ–π –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–∞—Ä—Ç–∏–Ω–æ–∫ (CIFAR-10 –∏ CIFAR-100). –ü–æ–ª—É—á–∏–ª–∏, —á—Ç–æ VCL –¥–æ–∫–∏–¥—ã–≤–∞–µ—Ç, –µ—Å–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Leaky ReLU –∏–ª–∏ ELU, –∞ –≤–æ—Ç —Å ReLU –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç batch normalization. –î–∞–ª–µ–µ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –≤ 2 —Ä–∞–∑–∞ –∏ –ø–µ—Ä–µ—Ö–æ–¥—è—Ç –Ω–∞ Tiny Imagenet ‚Äî —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Imagenet —Å 200 –∫–ª–∞—Å—Å–∞–º–∏ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 64x64. –ù–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ VCL –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç —É batch normalization –Ω–∞ —Å–µ—Ç–∫–µ —Å ELU, –∞ —Ç–∞–∫–∂–µ —É ResNet-110 –∏ DenseNet-40, –Ω–æ –ø—Ä–æ–∏–≥—Ä—ã–≤–∞–µ—Ç Wide-ResNet-32. –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –º–æ–º–µ–Ω—Ç —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª—É—á–∞—é—Ç—Å—è, –∫–æ–≥–¥–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ S1 –∏ S2 —Å–æ—Å—Ç–æ—è—Ç –∏–∑ –¥–≤—É—Ö —Å—ç–º–ø–ª–æ–≤. <br><br> –ü–æ–º–∏–º–æ —ç—Ç–æ–≥–æ –∞–≤—Ç–æ—Ä—ã —Ç–µ—Å—Ç–∏—Ä—É—é—Ç VCL –≤ feed-forward —Å–µ—Ç—è—Ö –∏ VCL –ø–æ–±–µ–∂–¥–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—â–µ, —á–µ–º —Å–µ—Ç—å —Å batch normalization –∏–ª–∏ –±–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏. <br><br> <b>DropMax: Adaptive Variational Softmax</b> <br><br>  <a href="https://arxiv.org/abs/1712.07834">Abstract</a> <br> <a href="https://github.com/haebeom-lee/dropmax">–ö–æ–¥</a> <br><br> –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –≤ –∑–∞–¥–∞—á–µ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞ —Å–ª—É—á–∞–π–Ω–æ –¥—Ä–æ–ø–∞—Ç—å –∫–∞–∫–æ–µ-—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–≤–µ—Ä–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤. –ü—Ä–∏—á–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —Å –∫–æ—Ç–æ—Ä–æ–π –º—ã –¥—Ä–æ–ø–Ω–µ–º —Ç–æ—Ç –∏–ª–∏ –∏–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è —Ç–æ–≥–æ –∏–ª–∏ –∏–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞, —Ç–∞–∫–∂–µ –æ–±—É—á–∞–µ—Ç—Å—è. –í –∏—Ç–æ–≥–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è, —á—Ç–æ —Å–µ—Ç—å ‚Äú–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–µ—Ç—Å—è‚Äù –Ω–∞ —Ç–æ–º, —á—Ç–æ–±—ã —Ä–∞–∑–ª–∏—á–∞—Ç—å —Å–∞–º—ã–µ —Ç—Ä—É–¥–Ω–æ–æ—Ç–¥–µ–ª–∏–º—ã–µ –∫–ª–∞—Å—Å—ã. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b2d/f45/914/b2df45914519d3b6dd1d4f12d5513775.png" alt="image"><br><br> –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ MNIST, CIFAR –∏ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞—Ö Imagenet –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DropMax —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ SoftMax, —Ç–∞–∫ –∏ —Ä—è–¥–∞ –µ–≥–æ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π. <br><br> <b>Accurate Intelligible Models with Pairwise Interactions</b> <br> (Friends Don't Let Friends Deploy Black-Box Models: The Importance of Intelligibility in Machine Learning) <br><br> Abstract: <a href="http://www.cs.cornell.edu/~yinlou/papers/lou-kdd13.pdf">http://www.cs.cornell.edu/~yinlou/papers/lou-kdd13.pdf</a> <br> –ö–æ–¥: –µ–≥–æ –Ω–µ—Ç. –ú–Ω–µ –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –∫–∞–∫ –≤ –≥–æ–ª–æ–≤–µ —É –∞–≤—Ç–æ—Ä–æ–≤ –≤—è–∂–µ—Ç—Å—è —Ç–∞–∫–æ–µ —Å–ª–µ–≥–∫–∞ –∏–º–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º –∫–æ–¥–∞. –ê–∫–∞–¥–µ–º–∏–∫–∏, —Å—ç—Ä =) <br><br> –ú–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —ç—Ç–æ—Ç –ø–∞–∫–µ—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä: <a href="https://github.com/dswah/pyGAM">https://github.com/dswah/pyGAM</a> . –í –Ω–µ–≥–æ –Ω–µ —Ç–∞–∫ –¥–∞–≤–Ω–æ –¥–æ–±–∞–≤–∏–ª–∏ feature interactions (—á—Ç–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç GAM –æ—Ç GA2M). <br><br> –≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–ª–∞—Å—å –≤ —Ä–∞–º–∫–∞—Ö –≤–æ—Ä–∫—à–æ–ø–∞ ‚ÄúInterpretability and Robustness in Audio, Speech, and Language‚Äù, —Ö–æ—Ç—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤ —Ü–µ–ª–æ–º, –∞ –Ω–µ –æ–±–ª–∞—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞ –∑–≤—É–∫–∞ –∏ —Ä–µ—á–∏.–ù–∞–≤–µ—Ä–Ω–æ–µ, –≤—Å–µ –≤ –∫–∞–∫–æ–π-—Ç–æ —Å—Ç–µ–ø–µ–Ω–∏ —Å—Ç–∞–ª–∫–∏–≤–∞–ª–∏—Å—å —Å –¥–∏–ª–µ–º–º–æ–π –≤—ã–±–æ—Ä–∞ –º–µ–∂–¥—É –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏ –∏ –µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏. –ï—Å–ª–∏ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é, –≤–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–∏ –º—ã –ø–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º –º–æ–∂–µ–º –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –∫–∞–∂–¥–∞—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –≤–ª–∏—è–µ—Ç –Ω–∞ –∑–∞–≤–∏—Å–∏–º—É—é. –ï—Å–ª–∏ –∂–µ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª–∏ ¬´—á–µ—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞¬ª, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏–ª–∏ –≥–ª—É–±–æ–∫–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±—É–¥–µ—Ç –æ—á–µ–Ω—å —Ç–æ—á–Ω–æ–π, –Ω–æ –æ—Ç—Å–ª–µ–¥–∏—Ç—å –∏ –æ–±—ä—è—Å–Ω–∏—Ç—å –≤—Å–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞ –≤ –¥–∞–Ω–Ω—ã—Ö, –±—É–¥–µ—Ç –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –±—É–¥–µ—Ç —Å–ª–æ–∂–Ω–æ –∏ –æ–±—ä—è—Å–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –∑–∞–∫–∞–∑—á–∏–∫—É, –∏ —Å–∞–º–∏–º –æ—Ç—Å–ª–µ–¥–∏—Ç—å, –Ω–µ –≤—ã—É—á–∏–ª–∞ –ª–∏ –æ–Ω–∞ —á—Ç–æ-—Ç–æ —Ç–∞–∫–æ–µ, —á—Ç–æ –Ω–∞–º –±—ã –Ω–µ —Ö–æ—Ç–µ–ª–æ—Å—å. –í —Ç–∞–±–ª–∏—Ü–µ –Ω–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –æ—Ü–µ–Ω–∫–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–¥–æ–≤ –º–æ–¥–µ–ª–µ–π. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fca/429/548/fca42954836ae74af472d4baacf732f1.png" alt="image"><br><br> –ü—Ä–∏–º–µ—Ä —Å–∏—Ç—É–∞—Ü–∏–∏, –∫–æ–≥–¥–∞ –ø–ª–æ—Ö–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å–≤—è–∑–∞–Ω–∞ —Å –±–æ–ª—å—à–∏–º–∏ —Ä–∏—Å–∫–∞–º–∏: –Ω–∞ –æ–¥–Ω–æ–º –∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Ä–µ—à–∞–ª–∞—Å—å –∑–∞–¥–∞—á–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–∞—Ü–∏–µ–Ω—Ç–∞ —É–º–µ—Ä–µ—Ç—å –æ—Ç –ø–Ω–µ–≤–º–æ–Ω–∏–∏. –í –¥–∞–Ω–Ω—ã—Ö –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞—Å—å —Å–ª–µ–¥—É—é—â–∞—è –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å: –µ—Å–ª–∏ —É —á–µ–ª–æ–≤–µ–∫–∞ –µ—Å—Ç—å –±—Ä–æ–Ω—Ö–∏–∞–ª—å–Ω–∞—è –∞—Å—Ç–º–∞, —Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É–º–µ—Ä–µ—Ç—å –æ—Ç –ø–Ω–µ–≤–º–æ–Ω–∏–∏ –Ω–∏–∂–µ, —á–µ–º —É –ª—é–¥–µ–π –±–µ–∑ —ç—Ç–æ–≥–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è. –ö–æ–≥–¥–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±—Ä–∞—Ç–∏–ª–∏—Å—å –∫ –ø—Ä–∞–∫—Ç–∏–∫—É—é—â–∏–º –≤—Ä–∞—á–∞–º, –≤—ã—è—Å–Ω–∏–ª–æ—Å—å, —á—Ç–æ —Ç–∞–∫–∞—è –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –µ—Å—Ç—å, –ø–æ—Å–∫–æ–ª—å–∫—É –ª—é–¥–∏ —Å –∞—Å—Ç–º–æ–π –≤ —Å–ª—É—á–∞–µ –ø–Ω–µ–≤–º–æ–Ω–∏–∏ –ø–æ–ª—É—á–∞—é—Ç –Ω–∞–∏–±–æ–ª–µ–µ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–æ–º–æ—â—å –∏ —Å–∏–ª—å–Ω—ã–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞. –ï—Å–ª–∏ –±—ã –º—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–ª–∏ xgboost –Ω–∞ —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –æ–Ω —É–ª–æ–≤–∏–ª –±—ã —ç—Ç—É –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å, –∏ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –æ—Ç–Ω–æ—Å–∏–ª–∞ –±—ã –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ —Å –∞—Å—Ç–º–æ–π –∫ –≥—Ä—É–ø–ø–µ –Ω–∏–∑–∫–æ–≥–æ —Ä–∏—Å–∫–∞, –∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª–∞ –±—ã –¥–ª—è –Ω–∏—Ö –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –ª–µ—á–µ–Ω–∏—è. <br><br> –ê–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É, –∫–æ—Ç–æ—Ä–∞—è –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–∞, –∏ —Ç–æ—á–Ω–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ ‚Äî —ç—Ç–æ GA2M, –ø–æ–¥–≤–∏–¥ –æ–±–æ–±—â–µ–Ω–Ω—ã—Ö –∞–¥–¥–∏—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (Generalized additive models). <br><br> –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ GAM –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å, –∫–∞–∫ –¥–∞–ª—å–Ω–µ–π—à–µ–µ –æ–±–æ–±—â–µ–Ω–∏–µ GLM: –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—É–º–º—É, –∫–∞–∂–¥—ã–π —á–ª–µ–Ω –∫–æ—Ç–æ—Ä–æ–π –æ—Ç—Ä–∞–∂–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–π –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–∞ –∑–∞–≤–∏—Å–∏–º—É—é, –Ω–æ –≤–ª–∏—è–Ω–∏–µ –≤—ã—Ä–∞–∂–∞–µ—Ç—Å—è –Ω–µ –æ–¥–Ω–∏–º –≤–µ—Å–æ–≤—ã–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º, –∫–∞–∫ –≤ GLM, –∞ –≥–ª–∞–¥–∫–æ–π –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π (–∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫—É—Å–æ—á–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ‚Äì —Å–ø–ª–∞–π–Ω—ã –∏–ª–∏ –¥–µ—Ä–µ–≤—å—è –Ω–µ–±–æ–ª—å—à–æ–π –≥–ª—É–±–∏–Ω—ã, –≤ —Ç–æ–º —á–∏—Å–ª–µ ¬´–ø–Ω–∏¬ª). –ó–∞ —Å—á–µ—Ç —ç—Ç–æ–π –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ GAM –º–æ–≥—É—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è, —á–µ–º –ø—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å. –° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –≤—ã—É—á–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (—Ñ—É–Ω–∫—Ü–∏–∏) –º–æ–∂–Ω–æ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –ø—Ä–æ–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/643/828/ca8/643828ca8aaeb0f0728ba2d094de7e43.png" alt="image"><br><br> –û–¥–Ω–∞–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ GAM –≤—Å–µ –∂–µ —á–∞—Å—Ç–æ –Ω–µ –¥–æ—Ç—è–≥–∏–≤–∞—é—Ç –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ ¬´—á–µ—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞¬ª. –ß—Ç–æ–±—ã —ç—Ç–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–º–ø—Ä–æ–º–∏—Å—Å ‚Äì –¥–æ–±–∞–≤–∏—Ç—å –≤ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –ø–æ–º–∏–º–æ —Ñ—É–Ω–∫—Ü–∏–π –æ–¥–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π, –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—É–Ω–∫—Ü–∏–π –¥–≤—É—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö ‚Äì —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∫–æ—Ç–æ—Ä—ã—Ö –∑–Ω–∞—á–∏–º–æ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –ø–æ–ª—É—á–∞–µ—Ç—Å—è GA2M. <br><br> –°–Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–æ–∏—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è GAM (–±–µ–∑ —É—á–µ—Ç–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö), –∞ –∑–∞—Ç–µ–º –ø–æ—à–∞–≥–æ–≤–æ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –ø–∞—Ä—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –æ—Å—Ç–∞—Ç–∫–∏ GAM). –î–ª—è —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ —Ç—Ä—É–¥–æ–µ–º–∫–æ, –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è FAST, —Å –ø–æ–º–æ—â—å—é –∫–æ—Ç–æ—Ä–æ–≥–æ –º–æ–∂–Ω–æ –∑–∞—Ä–∞–Ω–µ–µ –æ—Ç–æ–±—Ä–∞—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–µ –ø–∞—Ä—ã –∏ –∏–∑–±–µ–∂–∞—Ç—å –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–±–æ—Ä–∞. <br><br> –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–±–∏—Ç—å—Å—è –∫–∞—á–µ—Å—Ç–≤–∞, –±–ª–∏–∑–∫–æ–≥–æ –∫ –º–æ–¥–µ–ª—è–º –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –í —Ç–∞–±–ª–∏—Ü–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –¥–æ–ª—è –æ—à–∏–±–æ–∫ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∞–¥–¥–∏—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º –ª–µ—Å–æ–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö, –∏ –≤ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Å–ª—É—á–∞–µ–≤ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è GA2M c FAST –∏ –¥–ª—è —Å–ª—É—á–∞–π–Ω—ã—Ö –ª–µ—Å–æ–≤ –∑–Ω–∞—á–∏–º–æ –Ω–µ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e5b/809/a50/e5b809a50067cbfcd2f7bbb04f98630e.png" alt="image"><br><br> –•–æ—Ç–µ–ª –±—ã –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç –∞–∫–∞–¥–µ–º–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ —Ç–æ–ø–∫—É —ç—Ç–∏ –≤–∞—à–∏ –±—É—Å—Ç–∏–Ω–≥–∏ –∏ –¥–∏–ø –ª–µ—Ä–Ω–∏–Ω–≥–∏. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–µ –±–æ–ª–µ–µ 20 —Ç—ã—Å—è—á –æ–±—ä–µ–∫—Ç–æ–≤ (–≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è UCI). –í–æ–∑–Ω–∏–∫–∞–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å: –Ω–µ—É–∂–µ–ª–∏ –≤ 2018 –≥–æ–¥—É –Ω–µ—Ç –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è –ø–æ–¥–æ–±–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –ú–æ–∂–Ω–æ –≤–µ–¥—å –ø–æ–π—Ç–∏ –¥–∞–ª—å—à–µ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å—Å—è –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏–∑ 50 –æ–±—ä–µ–∫—Ç–æ–≤ ‚Äî —Ç–∞–º –µ—Å—Ç—å —à–∞–Ω—Å, —á—Ç–æ –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –±—É–¥–µ—Ç –∑–Ω–∞—á–∏–º–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞. <br><br> –°–ª–µ–¥—É—é—â–∏–π –º–æ–º–µ–Ω—Ç ‚Äî —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è. –ù–∞ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—á–µ–Ω—å –ª–µ–≥–∫–æ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è –¥–∞–∂–µ –±–µ–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã, –≤–æ–∑–º–æ–∂–Ω–æ, —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∞ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ ‚Äî —ç—Ç–æ black-box –º–æ–¥–µ–ª–∏. –ü–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ –≤ —Å—Ç–∞—Ç—å–µ –æ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ –≥–æ–≤–æ—Ä—è—Ç –Ω–∏–≥–¥–µ, —Ö–æ—Ç—è –æ–Ω–∞, –æ—á–µ–≤–∏–¥–Ω–æ, –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞. <br><br> –ò –ø–æ—Å–ª–µ–¥–Ω–µ–µ, –ø—Ä–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å. –î–∞–∂–µ –ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã –µ—Å–ª–∏ —É –Ω–∞—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ö–æ–≥–¥–∞ —É –≤–∞—Å 10 —Ç—ã—Å—è—á –Ω–æ—Ä–º–∞–ª—å–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤ (–≤ —Å–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫ –∏ –±—É–¥–µ—Ç), –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å, –∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã –∑–∞ —Ç–æ, —á—Ç–æ predict_proba –≤—ã–¥–∞–µ—Ç 0.86. –î–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –º—ã —Ö–æ—Ç–∏–º –Ω–µ –ø—Ä–æ—Å—Ç–æ –ª–∏–Ω–µ–π–Ω—É—é –º–æ–¥–µ–ª—å, –∞ –ª–∏–Ω–µ–π–Ω—É—é –º–æ–¥–µ–ª—å —Å–æ sparse –≤–µ—Å–∞–º–∏. –ö–∞–∑–∞–ª–æ—Å—å –±—ã, —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ –¥–æ–±–∏—Ç—å—Å—è L1-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π, –Ω–æ –∑–¥–µ—Å—å —Ç–æ–∂–µ –Ω–µ –≤—Å–µ —Ç–∞–∫ –ø—Ä–æ—Å—Ç–æ. –ò–∑ –Ω–∞–±–æ—Ä–∞ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏—á–µ–π L1-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤—ã–±–µ—Ä–µ—Ç –æ–¥–Ω—É –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Å–ª—É—á–∞–π–Ω–æ. –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—É—á–∞—Ç –≤–µ—Å 0, —Ö–æ—Ç—è, –µ—Å–ª–∏ –æ–¥–Ω–∞ –∏–∑ —ç—Ç–∏—Ö —Ñ–∏—á–µ–π –æ–±–ª–∞–¥–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é, –¥—Ä—É–≥–∏–µ —è–≤–Ω–æ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –ø—Ä–æ—Å—Ç–æ —à—É–º–æ–º. –í –ø–ª–∞–Ω–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ –û–ö, –≤ –ø–ª–∞–Ω–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —ç—Ç–æ –æ—á–µ–Ω—å –ø–ª–æ—Ö–æ. –¢–æ –µ—Å—Ç—å –¥–∞–∂–µ —Å –ª–∏–Ω–µ–π–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–µ –≤—Å–µ —Ç–∞–∫ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ, –ø–æ–¥—Ä–æ–±–Ω–µ–µ –æ–± interpretable and credible models –º–æ–∂–Ω–æ –≥–ª—è–Ω—É—Ç—å <a href="https://arxiv.org/abs/1711.03190">–∑–¥–µ—Å—å</a> . <br><br> <b>Visualization for Machine Learning: UMAP</b> <br><br> <a href="https://arxiv.org/abs/1802.03426">Absract</a> <br> <a href="https://github.com/lmcinnes/umap">–ö–æ–¥</a> <br><br> –í –¥–µ–Ω—å —Ç—å—é—Ç–æ—Ä–∏–∞–ª–æ–≤ –æ–¥–Ω–∏–º –∏–∑ –ø–µ—Ä–≤—ã—Ö —à–ª–æ –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ ‚ÄúVisualization for Machine Learning‚Äù –æ—Ç Google Brain. –í —Ä–∞–º–∫–∞—Ö —Ç—å—é—Ç–æ—Ä–∏–∞–ª–∞ –Ω–∞–º —Ä–∞—Å—Å–∫–∞–∑–∞–ª–∏ –ø—Ä–æ –∏—Å—Ç–æ—Ä–∏—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π, –Ω–∞—á–∏–Ω–∞—è –æ—Ç —Å–æ–∑–¥–∞—Ç–µ–ª—è –ø–µ—Ä–≤—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ —Ä–∞–∑–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º–æ–∑–≥–∞ –∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ —Ç–µ—Ö–Ω–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, —á—Ç–æ–±—ã –ø—Ä–∏–≤–ª–µ—á—å –≤–Ω–∏–º–∞–Ω–∏–µ –∫ —Å–∞–º–æ–º—É –≤–∞–∂–Ω–æ–º—É –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ, –ø—É—Å—Ç—å –¥–∞–∂–µ —Å–æ–¥–µ—Ä–∂–∞—â–µ–π –º–Ω–æ–≥–æ –º–µ–ª–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π ‚Äì –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–æ–π, —Ü–≤–µ—Ç–æ–º, —Ä–∞–º–æ—á–∫–æ–π –∏ —Ç.–¥., –∫–∞–∫ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ –Ω–∏–∂–µ. –Ø —ç—Ç—É —á–∞—Å—Ç—å –ø—Ä–æ–ø—É—â—É, –Ω–æ —Ç—É—Ç –µ—Å—Ç—å <a href="https://hub.packtpub.com/neurips-2018-a-quick-look-at-data-visualization-for-machine-learning-by-google-pair-researchers-tutorial/">—Ö–æ—Ä–æ—à–∏–π –æ–±–∑–æ—Ä</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/751/f0f/c31/751f0fc318a4bb3edadf59e35ecc7ba9.png" alt="image"><br><br> –õ–∏—á–Ω–æ –º–Ω–µ –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –±—ã–ª–∞ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞ —Ç–µ–º–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ –ø–æ–¥—Ö–æ–¥ Uniform Manifold Approximation and Projection (UMAP) ‚Äì –Ω–æ–≤—ã–π –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–π –º–µ—Ç–æ–¥ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –æ–Ω –±—ã–ª –≤ —Ñ–µ–≤—Ä–∞–ª–µ —ç—Ç–æ–≥–æ –≥–æ–¥–∞, –ø–æ—ç—Ç–æ–º—É –º–∞–ª–æ –∫—Ç–æ –ø–æ–∫–∞ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç, –Ω–æ –≤—ã–≥–ª—è–¥–∏—Ç –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–µ –∫–∞–∫ –≤ –ø–ª–∞–Ω–µ –≤—Ä–µ–º–µ–Ω–∏ —Ä–∞–±–æ—Ç—ã, —Ç–∞–∫ –∏ –≤ –ø–ª–∞–Ω–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–≤—É—Ö–º–µ—Ä–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è—Ö. –¢–∞–∫, –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã UMAP –æ–ø–µ—Ä–µ–∂–∞–µ—Ç t-SNE –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –≤ 2-10 —Ä–∞–∑, –ø—Ä–∏—á–µ–º —á–µ–º –±–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, —Ç–µ–º –±–æ–ª—å—à–µ –æ—Ç—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b82/045/a12/b82045a12b1cb8782c5c9fb64dfa46b9.png" alt="image"><br><br> –ö—Ä–æ–º–µ —ç—Ç–æ–≥–æ, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç t-SNE –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã UMAP –ø–æ—á—Ç–∏ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞, –≤ –∫–æ—Ç–æ—Ä–æ–µ –º—ã —ç–º–±–µ–¥–¥–∏–º –Ω–∞—à –¥–∞—Ç–∞—Å–µ—Ç (—Å–º. —Ä–∏—Å. –Ω–∏–∂–µ), —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á (–ø–æ–º–∏–º–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏) ‚Äî –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0c6/f14/56a/0c6f1456a68537b5e12f58f6e1dfb740.png" alt="image"><br><br> –ü—Ä–∏ —ç—Ç–æ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ UMAP —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ —Ö—É–∂–µ, –∞ –º–µ—Å—Ç–∞–º–∏ –ª—É—á—à–µ t-SNE: –Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö MNIST –∏ Fashion MNIST –∫–ª–∞—Å—Å—ã –ª—É—á—à–µ —Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –∏–º–µ–Ω–Ω–æ –≤ –≤–∞—Ä–∏–∞–Ω—Ç–µ —Å UMAP: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f35/4b4/869/f354b48691226fd262e3f92af9fe398f.png" alt="image"><br><br> –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–ª—é—Å ‚Äî —É–¥–æ–±–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è: –∫–ª–∞—Å—Å UMAP –Ω–∞—Å–ª–µ–¥—É–µ—Ç—Å—è –æ—Ç –∫–ª–∞—Å—Å–æ–≤ sklearn, —Ç–∞–∫ —á—Ç–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –≤ sklearn pipeline. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è, —á—Ç–æ UMAP –±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º, —á–µ–º t-SNE, —Ç.–∫. –ª—É—á—à–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö. <br><br> –í –±—É–¥—É—â–µ–º –∞–≤—Ç–æ—Ä—ã –ø–ª–∞–Ω–∏—Ä—É—é—Ç –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É semi-supervised –æ–±—É—á–µ–Ω–∏—è ‚Äî —Ç–æ –µ—Å—Ç—å –µ—Å–ª–∏ —É –Ω–∞—Å –±—É–¥—É—Ç –º–µ—Ç–∫–∏ —Ö–æ—Ç—è –±—ã –¥–ª—è —á–∞—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–æ–≤, –º—ã –º–æ–∂–µ–º –ø–æ—Å—Ç—Ä–æ–∏—Ç—å UMAP —Å —É—á–µ—Ç–æ–º —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. <br><br> –ê –∫–∞–∫–∏–µ —Å—Ç–∞—Ç—å–∏ –ø–æ–Ω—Ä–∞–≤–∏–ª–∏—Å—å –≤–∞–º? –ü–∏—à–∏—Ç–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã, –º—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω–∞ –Ω–∏—Ö –æ—Ç–≤–µ—Ç–∏–º. </div><p>Source: <a href="https://habr.com/ru/post/434694/">https://habr.com/ru/post/434694/</a></p>
<section class="navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><p>Waiting for the list form <a href="../../index.html">here</a>...</p></nav>
</section>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>
</body>

</html>